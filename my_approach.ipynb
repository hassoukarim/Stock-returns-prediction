{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qube research and technologies data challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this challenge is to try to predict the returns of 50 stocks.\n",
    "\n",
    "In a classical approach: \n",
    "- An autoregressive model could be used.\n",
    "- An LSTM or attention based model could be used.\n",
    "- Some manually designed features could be considered and a linear regression using MSE could be used to predict the returns.\n",
    "\n",
    "However in the scope of this challenge, the following approach was imposed:\n",
    "\n",
    "- Try to predict the return of a stock if you are given the returns of the $D$ previous days ($D$ being imposed to be equal to 250 in this challenge).\n",
    "- Thus, your initial features for each stock $S$ and day $t$ would be the returns of the stock in question between the days $t - 1$ and $t - 250$.\n",
    "- Given a matrix $M \\in M_{50 \\times T, 250}(\\mathbb{R})$ where each line $i$ of this matrix consists of the returns of the stock $S_{i~mod~50}$ from day $\\lfloor \\frac{i}{50} \\rfloor + 250 - 1$ to day $\\lfloor \\frac{i}{50} \\rfloor + 250 - 250$, come up with a new feature space of dimension 10 such that the transformed feature matrix $F = MA$ where $A \\in M_{250, 10}(\\mathbb(R)$ is a learnable orthonormal matrix. (i.e $AA^{T} = I$).\n",
    "- Once $F$ is determined, perform a classical linear regression on the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.optimize import minimize, NonlinearConstraint\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import linear_model\n",
    "import torch\n",
    "from numdifftools import Jacobian\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_train.csv', index_col=0, sep=',')\n",
    "X.columns.name = 'date'\n",
    "\n",
    "Y = pd.read_csv('Y_train.csv', index_col=0, sep=',')\n",
    "Y.columns.name = 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stocksID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.018647</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>-0.016676</td>\n",
       "      <td>-0.005110</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>-0.011745</td>\n",
       "      <td>0.007120</td>\n",
       "      <td>-0.008451</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012525</td>\n",
       "      <td>-0.011716</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.018142</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.002732</td>\n",
       "      <td>0.013074</td>\n",
       "      <td>-0.005843</td>\n",
       "      <td>-0.003823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008254</td>\n",
       "      <td>-0.022280</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>-0.006820</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>-0.002255</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>-0.004672</td>\n",
       "      <td>-0.002889</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>0.005005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.008404</td>\n",
       "      <td>-0.013629</td>\n",
       "      <td>-0.006044</td>\n",
       "      <td>-0.003425</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006245</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>-0.007338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.022734</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>-0.008568</td>\n",
       "      <td>-0.010899</td>\n",
       "      <td>-0.017981</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>-0.011980</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>-0.010636</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.013369</td>\n",
       "      <td>0.019738</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>-0.003669</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>-0.008150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.024546</td>\n",
       "      <td>-0.008315</td>\n",
       "      <td>-0.007991</td>\n",
       "      <td>-0.003515</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>-0.004614</td>\n",
       "      <td>-0.008182</td>\n",
       "      <td>-0.005255</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017507</td>\n",
       "      <td>-0.001233</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.009262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date             0         1         2         3         4         5  \\\n",
       "stocksID                                                               \n",
       "0        -0.018647 -0.013002 -0.010776 -0.016676 -0.005110  0.009092   \n",
       "1        -0.008254 -0.022280  0.012173 -0.006820 -0.004055  0.012912   \n",
       "2        -0.008404 -0.013629 -0.006044 -0.003425 -0.009522 -0.001353   \n",
       "3        -0.022734 -0.006981 -0.008568 -0.010899 -0.017981  0.002485   \n",
       "4        -0.024546 -0.008315 -0.007991 -0.003515  0.007872  0.007082   \n",
       "\n",
       "date             6         7         8         9  ...       744       745  \\\n",
       "stocksID                                          ...                       \n",
       "0        -0.011745  0.007120 -0.008451  0.009119  ... -0.012525 -0.011716   \n",
       "1        -0.001293  0.009994 -0.002747  0.001664  ...  0.014432 -0.002255   \n",
       "2        -0.000637  0.007640  0.001600  0.007416  ... -0.006245 -0.001329   \n",
       "3        -0.011980  0.012446 -0.010636  0.003807  ... -0.005179 -0.003442   \n",
       "4        -0.004614 -0.008182 -0.005255  0.014404  ... -0.017507 -0.001233   \n",
       "\n",
       "date           746       747       748       749       750       751  \\\n",
       "stocksID                                                               \n",
       "0         0.003532  0.009965  0.018142 -0.001236 -0.002732  0.013074   \n",
       "1        -0.011493  0.002291 -0.001346 -0.004026 -0.004672 -0.002889   \n",
       "2         0.005230  0.003510  0.006022 -0.000343  0.001757  0.004972   \n",
       "3         0.002733  0.013369  0.019738  0.001201 -0.003669  0.008690   \n",
       "4        -0.000552  0.004664  0.005202  0.007695  0.003775  0.005097   \n",
       "\n",
       "date           752       753  \n",
       "stocksID                      \n",
       "0        -0.005843 -0.003823  \n",
       "1        -0.004984  0.005005  \n",
       "2         0.004916 -0.007338  \n",
       "3         0.000272 -0.008150  \n",
       "4         0.001135 -0.009262  \n",
       "\n",
       "[5 rows x 754 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>...</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stocksID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001128</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.007027</td>\n",
       "      <td>-0.009757</td>\n",
       "      <td>-0.005868</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012525</td>\n",
       "      <td>-0.011716</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.018142</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.002732</td>\n",
       "      <td>0.013074</td>\n",
       "      <td>-0.005843</td>\n",
       "      <td>-0.003823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003274</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>-0.012834</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>-0.009097</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>-0.002255</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>-0.004672</td>\n",
       "      <td>-0.002889</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>0.005005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003446</td>\n",
       "      <td>-0.001272</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>-0.006994</td>\n",
       "      <td>-0.005512</td>\n",
       "      <td>-0.003652</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>-0.004550</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006245</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>-0.007338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>-0.022219</td>\n",
       "      <td>-0.017467</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>-0.003512</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.013369</td>\n",
       "      <td>0.019738</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>-0.003669</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>-0.008150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>-0.001487</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>-0.006636</td>\n",
       "      <td>-0.012548</td>\n",
       "      <td>-0.000942</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>-0.018902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017507</td>\n",
       "      <td>-0.001233</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.009262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date           250       251       252       253       254       255  \\\n",
       "stocksID                                                               \n",
       "0         0.001128 -0.001046 -0.007027 -0.009757 -0.005868 -0.008563   \n",
       "1        -0.003274 -0.001070  0.000205  0.004862 -0.012834 -0.004868   \n",
       "2        -0.003446 -0.001272  0.005824 -0.006994 -0.005512 -0.003652   \n",
       "3         0.001727  0.003143 -0.003606 -0.022219 -0.017467 -0.004536   \n",
       "4         0.004907  0.002472 -0.001487  0.003317 -0.006636 -0.012548   \n",
       "\n",
       "date           256       257       258       259  ...       744       745  \\\n",
       "stocksID                                          ...                       \n",
       "0        -0.005857 -0.004588 -0.000240  0.008705  ... -0.012525 -0.011716   \n",
       "1         0.005476 -0.009097  0.000284  0.002971  ...  0.014432 -0.002255   \n",
       "2         0.003997 -0.005139 -0.004550  0.001228  ... -0.006245 -0.001329   \n",
       "3        -0.003512 -0.000156  0.005489  0.005115  ... -0.005179 -0.003442   \n",
       "4        -0.000942 -0.009569  0.004660 -0.018902  ... -0.017507 -0.001233   \n",
       "\n",
       "date           746       747       748       749       750       751  \\\n",
       "stocksID                                                               \n",
       "0         0.003532  0.009965  0.018142 -0.001236 -0.002732  0.013074   \n",
       "1        -0.011493  0.002291 -0.001346 -0.004026 -0.004672 -0.002889   \n",
       "2         0.005230  0.003510  0.006022 -0.000343  0.001757  0.004972   \n",
       "3         0.002733  0.013369  0.019738  0.001201 -0.003669  0.008690   \n",
       "4        -0.000552  0.004664  0.005202  0.007695  0.003775  0.005097   \n",
       "\n",
       "date           752       753  \n",
       "stocksID                      \n",
       "0        -0.005843 -0.003823  \n",
       "1        -0.004984  0.005005  \n",
       "2         0.004916 -0.007338  \n",
       "3         0.000272 -0.008150  \n",
       "4         0.001135 -0.009262  \n",
       "\n",
       "[5 rows x 504 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful data remodeling\n",
    "\n",
    "Multi level indexing where:\n",
    "\n",
    "Let $D_t$ be the matrix of $M_{50, 250}(\\mathbb{R})$ which columns are the returns for each of the 50 stocks during the previou 250 days $R_{t-i}$ for $i \\in \\{ 1,...,250\\}$.\n",
    "\n",
    "Then, we create X_train_reshape to be the vertical concatenation of the $D_t$ matrixes.\n",
    "\n",
    "Therefore, the features matrix $F$ will be the vertical concatenation of the products $D_t M$ with $M$ being the Stiefel matrix we considered for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshape = pd.concat([ X.T.shift(i+1).stack(dropna=False) for i in range(250) ], axis=1).dropna()\n",
    "X_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "Y_reshape = Y.T.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeLag</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>stocksID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">250</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.011243</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>-0.008509</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>-0.018546</td>\n",
       "      <td>-0.008353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>-0.008451</td>\n",
       "      <td>0.007120</td>\n",
       "      <td>-0.011745</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>-0.005110</td>\n",
       "      <td>-0.016676</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>-0.018647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000982</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>-0.003902</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.002155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>-0.006820</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>-0.022280</td>\n",
       "      <td>-0.008254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>-0.006942</td>\n",
       "      <td>-0.013340</td>\n",
       "      <td>-0.008071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>-0.003425</td>\n",
       "      <td>-0.006044</td>\n",
       "      <td>-0.013629</td>\n",
       "      <td>-0.008404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006515</td>\n",
       "      <td>-0.006553</td>\n",
       "      <td>0.009464</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>-0.005438</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>-0.012432</td>\n",
       "      <td>-0.006100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>-0.010636</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>-0.011980</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>-0.017981</td>\n",
       "      <td>-0.010899</td>\n",
       "      <td>-0.008568</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>-0.022734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.006223</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.007609</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>0.039274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>-0.005255</td>\n",
       "      <td>-0.008182</td>\n",
       "      <td>-0.004614</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>-0.003515</td>\n",
       "      <td>-0.007991</td>\n",
       "      <td>-0.008315</td>\n",
       "      <td>-0.024546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "timeLag             1         2         3         4         5         6    \\\n",
       "date stocksID                                                               \n",
       "250  0         0.000103  0.012387  0.011243  0.002595 -0.008509 -0.002711   \n",
       "     1        -0.000982  0.003932  0.000050  0.001616 -0.003902 -0.001686   \n",
       "     2         0.009301  0.003914  0.004995  0.001539  0.001452  0.002809   \n",
       "     3         0.006515 -0.006553  0.009464  0.005204  0.004227 -0.005438   \n",
       "     4        -0.006223  0.005415  0.014643  0.005195  0.004489  0.002695   \n",
       "\n",
       "timeLag             7         8         9         10   ...       241  \\\n",
       "date stocksID                                          ...             \n",
       "250  0         0.008934  0.006571 -0.018546 -0.008353  ...  0.009119   \n",
       "     1         0.008810  0.001585 -0.000745 -0.002155  ...  0.001664   \n",
       "     2         0.005177 -0.006942 -0.013340 -0.008071  ...  0.007416   \n",
       "     3         0.008861  0.004025 -0.012432 -0.006100  ...  0.003807   \n",
       "     4         0.007609  0.011437 -0.004804  0.039274  ...  0.014404   \n",
       "\n",
       "timeLag             242       243       244       245       246       247  \\\n",
       "date stocksID                                                               \n",
       "250  0        -0.008451  0.007120 -0.011745  0.009092 -0.005110 -0.016676   \n",
       "     1        -0.002747  0.009994 -0.001293  0.012912 -0.004055 -0.006820   \n",
       "     2         0.001600  0.007640 -0.000637 -0.001353 -0.009522 -0.003425   \n",
       "     3        -0.010636  0.012446 -0.011980  0.002485 -0.017981 -0.010899   \n",
       "     4        -0.005255 -0.008182 -0.004614  0.007082  0.007872 -0.003515   \n",
       "\n",
       "timeLag             248       249       250  \n",
       "date stocksID                                \n",
       "250  0        -0.010776 -0.013002 -0.018647  \n",
       "     1         0.012173 -0.022280 -0.008254  \n",
       "     2        -0.006044 -0.013629 -0.008404  \n",
       "     3        -0.008568 -0.006981 -0.022734  \n",
       "     4        -0.007991 -0.008315 -0.024546  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date  stocksID\n",
       "250   0           0.001128\n",
       "      1          -0.003274\n",
       "      2          -0.003446\n",
       "      3           0.001727\n",
       "      4           0.004907\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_reshape.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check if a matrix is a Stiefel matrix up to an error factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOrthonormality(A): \n",
    "    \n",
    "    bool = True\n",
    "    D, F = A.shape   \n",
    "    Error = pd.DataFrame(A.T @ A - np.eye(F)).abs()\n",
    "    \n",
    "    if any(Error.unstack() > 1e-6):\n",
    "        bool = False\n",
    "     \n",
    "    return bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function below computes the metric that assesses the quality of $A$ and $\\beta$ on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(A, beta, X, y): \n",
    "    \n",
    "    if not checkOrthonormality(A):\n",
    "        return -1.0    \n",
    "    \n",
    "    Ypred = (X @ A @ beta).unstack().T         \n",
    "    Ytrue = y\n",
    "    \n",
    "    \n",
    "    Ytrue = Ytrue.div(np.sqrt((Ytrue**2).sum()), 1)    \n",
    "    Ypred = Ypred.div(np.sqrt((Ypred**2).sum()), 1)\n",
    "\n",
    "    meanOverlap = (Ytrue * Ypred).sum().mean()\n",
    "\n",
    "    return  meanOverlap  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding $\\beta$ for least square errors regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitBeta(A, X, y):\n",
    "    \n",
    "    predictors = X @ A\n",
    "    targets = y\n",
    "    beta = np.linalg.inv(predictors.T @ predictors) @ predictors.T @ targets\n",
    "    \n",
    "    return beta.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First idea: Convex optimization\n",
    "\n",
    "We could consider this problem as a pure optimization problem with constraints over $A$.\n",
    "\n",
    "We know that our predictions are given by the formula (here $X = X_{reshape}$):\n",
    "\n",
    "$Y_{pred} = XA\\beta$\n",
    "\n",
    "$XA = (\\sum_{0 \\leq l \\leq 249} X_{il} A_{lj})_{ij}$\n",
    "\n",
    "let $C = XA$\n",
    "\n",
    "then:\n",
    "\n",
    "$XA\\beta = C\\beta = (\\sum_{0 \\leq j \\leq 9} C_{ij} \\beta_{j})_{i} = (\\sum_{0 \\leq j \\leq 9} \\sum_{0 \\leq l \\leq 249} X_{il} A_{lj} \\beta_{j})_{i}$\n",
    "\n",
    "Therefore using the mean squared error as our loss function, we arrive at the following optimization problem:\n",
    "\n",
    "$argmin_{A, \\beta}\\frac{1}{25200}\\sum_{0 \\leq i \\leq 24199} (\\sum_{0 \\leq j \\leq 9} \\sum_{0 \\leq l \\leq 249} X_{il} A_{lj} \\beta_{j} - y_{i})^{2}$\n",
    "\n",
    "with the constraint that $A^TA = I$\n",
    "\n",
    "that we could formalize as follows:\n",
    "\n",
    "$\\forall i \\in \\{0,..,9\\}\\forall j \\in \\{0,..,9\\} \\sum_{0 \\leq l \\leq 249}A_{li}A_{lj} = \\delta_{ij}$\n",
    "\n",
    "As much as I would love to introduce the lagrange multipliers and perform a stochastic gradient descent by hands. The task seems tedious. Therefore, I will look into some optimization libraries in order to solve for my parameters.\n",
    "\n",
    "Little optimization problem:\n",
    "It seems that when optimizing, the algorithm converges to a minimum that's not a point in the feasible set.(Though we could argue it's close enough to be in it). We take that point and shift it a little bit to be in the feasible by autonormalizing A and fitting beta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x):\n",
    "    \"\"\"\n",
    "        Let's note that x is a 1D vector where:\n",
    "        - the first 10 elements will be the elements of beta (x[:10])\n",
    "        - the next 250 * 10 elements will be the elements of A flattened row wise.\n",
    "        (meaning that x[10] is the first column of the first line of A and x[20] is the\n",
    "        first column of the second row of A)\n",
    "    \"\"\"\n",
    "    pseudo_beta = x[:10]\n",
    "    pseudo_A = np.reshape(x[10:], (250, 10))\n",
    "    Ypred = (X_reshape @ pseudo_A @ pseudo_beta).unstack().T         \n",
    "    Ytrue = Y_reshape\n",
    "    return ((Ypred - Ytrue)**2).sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our contraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cons = []\n",
    "for i in range(9):\n",
    "    for j in range(i, 9):\n",
    "        list_cons.append(NonlinearConstraint(lambda x: sum([x[10+l*10+i]*x[10+l*10+j] for l in range(249)])\\\n",
    "                                             - float(i == j), -1e-6, 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our starting optimization points\n",
    "\n",
    "We choose to start with a decent feasible solution. Since the constraints are on $A$, we choose to initialize $A$ with the matrix composed of the 10 first principal components of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "scaler = StandardScaler()\n",
    "pca.fit(scaler.fit_transform(X_reshape))\n",
    "begin_A = pca.components_[0:10].T\n",
    "begin_A = begin_A.reshape((10*250, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_beta = np.array([0.3 for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93293/2763600814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'SLSQP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         return _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    632\u001b[0m                                constraints, callback=callback, **options)\n\u001b[1;32m    633\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/slsqp.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;31m# ScalarFunction provides function and gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     sf = _prepare_scalar_function(func, x, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m    376\u001b[0m                                   \u001b[0mfinite_diff_rel_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinite_diff_rel_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                                   bounds=new_bounds)\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    262\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Hessian Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n\u001b[0m\u001b[1;32m    156\u001b[0m                                            **finite_diff_options)\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[1;32m    487\u001b[0m                                      use_one_sided, method)\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3-point'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_93293/284530182.py\u001b[0m in \u001b[0;36mloss_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mYpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_reshape\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mpseudo_A\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mpseudo_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mYtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mYtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  10706\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10707\u001b[0m         ):\n\u001b[0;32m> 10708\u001b[0;31m             return NDFrame.sum(\n\u001b[0m\u001b[1;32m  10709\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10710\u001b[0m             )\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  10444\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10445\u001b[0m     ):\n\u001b[0;32m> 10446\u001b[0;31m         return self._min_count_stat_function(\n\u001b[0m\u001b[1;32m  10447\u001b[0m             \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnansum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10448\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_min_count_stat_function\u001b[0;34m(self, name, func, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  10426\u001b[0m                 \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10427\u001b[0m             )\n\u001b[0;32m> 10428\u001b[0;31m         return self._reduce(\n\u001b[0m\u001b[1;32m  10429\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10430\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   9791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9792\u001b[0m         dtype_is_dt = np.array(\n\u001b[0;32m-> 9793\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mis_datetime64_any_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mown_dtypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9794\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9795\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   9791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9792\u001b[0m         dtype_is_dt = np.array(\n\u001b[0;32m-> 9793\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mis_datetime64_any_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mown_dtypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9794\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9795\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res_opt = minimize(loss_func, np.concatenate((begin_beta, begin_A)), method ='SLSQP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res_opt.x\n",
    "opt_beta = res[:10]\n",
    "opt_A = np.reshape(res[10:], (250, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the solution feasible\n",
    "\n",
    "NB : We didnt include the constraints in the optimizer because they slowed the optimization process without even leading to a final feasible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.linalg.qr(opt_A)[0]\n",
    "beta = fitBeta(opt_A, X_reshape, Y_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metric_train(A, beta, X_reshape, Y_reshape)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second idea: Using an MLP and the Adam optimizer\n",
    "### Defining the model to be optimized\n",
    "\n",
    "The model will be a multilayer perceptron where the first layer is the learnable Stiefel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QrtModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QrtModel, self).__init__()\n",
    "        self.A = torch.nn.Linear(in_features=250, out_features=10, bias=False)\n",
    "        self.beta = torch.nn.Linear(in_features=10, out_features=1, bias=False)\n",
    "    def forward(self, x):\n",
    "        new_features = self.A(x)\n",
    "        final_pred = self.beta(new_features)\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the considered neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data = X_reshape.merge(Y_reshape.rename(\"y\"), left_index=True, right_index=True)\n",
    "full_train_data_to_tensor = torch.tensor(full_train_data.values)\n",
    "batches_iterator = torch.utils.data.DataLoader(full_train_data_to_tensor, batch_size=50)\n",
    "qrt_model = QrtModel()\n",
    "opt = torch.optim.Adam(qrt_model.parameters(), lr=0.00001)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "N_EPOCHS = 150\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for batch in batches_iterator:\n",
    "        x_batch = batch[:, :-1].to(torch.float32)\n",
    "        y_batch = batch[:, -1].to(torch.float32)\n",
    "        opt.zero_grad()\n",
    "        pred = qrt_model(x_batch)\n",
    "        n_lines = pred.shape[0]\n",
    "        pred = torch.reshape(pred, (n_lines,))\n",
    "        loss = loss_function(pred, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "for param in qrt_model.A.parameters():\n",
    "    A = np.transpose(param.detach().numpy())\n",
    "    A = np.linalg.qr(A)[0]\n",
    "\n",
    "#for param in qrt_model.beta.parameters():\n",
    "#    beta = param.detach().numpy()[0]\n",
    "\n",
    "beta = fitBeta(A, X_reshape, Y_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12945404773905905\n"
     ]
    }
   ],
   "source": [
    "print(metric(A, beta, X_reshape, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the shape of the matrix $A$ and $\\beta$ to the shape requested in the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametersTransform(A, beta, D=250, F=10):\n",
    "    \n",
    "    if A.shape != (D, F):\n",
    "        print('A has not the good shape')\n",
    "        return\n",
    "    \n",
    "    if beta.shape[0] != F:\n",
    "        print('beta has not the good shape')\n",
    "        return        \n",
    "    \n",
    "    output = np.hstack( (np.hstack([A.T, beta.reshape((F, 1))])).T )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### puts the output in a csv file to be later submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from output to csv file...\n",
    "output = parametersTransform(A, beta)\n",
    "pd.DataFrame(output).to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
